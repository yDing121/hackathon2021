{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tommysucks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSPvLt8PfmiX"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from importlib.machinery import SourceFileLoader\n",
        "from os.path import join\n",
        "from torchtext.vocab import GloVe\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.stem.porter import *\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords' ,quiet=True)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn import metrics\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dropout\n",
        "\n",
        "import re\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from google.colab import files, drive\n",
        "import os"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ-m56zvxDfp",
        "outputId": "06ce3b6c-0c21-42e2-e359-b9ad3b4c7cef"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FD4GsNGxTv1",
        "outputId": "747b323e-1b2e-4126-cc27-36b9cc50b038"
      },
      "source": [
        "% cd drive/MyDrive"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive'\n",
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayLYexnPr1Q5",
        "outputId": "026223a2-599b-4d42-eec8-43a31c605a41"
      },
      "source": [
        "# Read csvs\n",
        "df_train = pd.read_csv(\"finance_train.csv\")\n",
        "df_test = pd.read_csv(\"finance_test.csv\")\n",
        "\n",
        "# Constants\n",
        "PUNCTUATION = '!#$%&()*,-./:;<=>?@^_`{|}~'\n",
        "PUNCTUATION_RE = re.compile(\"[%s]\" % PUNCTUATION)\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "MAX_NB_WORDS = None\n",
        "MAX_SEQUENCE_LENGTH = None\n",
        "EPOCHS = None\n",
        "EMBEDDING_DIM = None\n",
        "BATCH_SIZE = None\n",
        "\n",
        "# Clean text - returns lowercase text with removed chars and stopwords\n",
        "def clean_text(text:str):\n",
        "    text = text.lower()\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "    text = BAD_SYMBOLS_RE.sub('', text)\n",
        "    text = text.replace('x', '')\n",
        "    text = PUNCTUATION_RE.sub('', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) \n",
        "    return text\n",
        "\n",
        "# Padding and indexing of unique words\n",
        "def pad_sequences_train(train, test):\n",
        "  tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters=PUNCTUATION, lower=True)\n",
        "  tokenizer.fit_on_texts(train['Sentence'].values)\n",
        "  word_index = tokenizer.word_index\n",
        "  X = tokenizer.texts_to_sequences(train['Sentence'].values)\n",
        "  X_train = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  Xt = tokenizer.texts_to_sequences(test['Sentence'].values)\n",
        "  X_test = pad_sequences(Xt, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  return X_train, X_test\n",
        "\n",
        "# Run model\n",
        "def run_model(xtr, ytr, xt, yt, labelnum, epochs=5, max_sequence_length=256, max_nb_words=1000, embedding_dim=300):\n",
        "  # Problematic input\n",
        "  if any(x is None for x in [xtr, ytr, xt, yt, epochs, max_sequence_length, max_nb_words, embedding_dim]):\n",
        "    print('Replace the None values above with your new values before calling the run_model() function.')\n",
        "    return None, None, None\n",
        "  \n",
        "  # NN\n",
        "  model = Sequential() # Container\n",
        "  model.add(Embedding(max_nb_words+1, embedding_dim, mask_zero=True, input_length=max_sequence_length)) # Embedding\n",
        "  model.add(SpatialDropout1D(0.2)) # Dropout\n",
        "  model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) # LSTM layer\n",
        "  model.add(Dense(labelnum, activation='softmax')) # Densely connected layer with softmax activation\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  # Performance\n",
        "  print(model.summary())\n",
        "  history = model.fit(xtr, \n",
        "                    ytr, \n",
        "                    epochs=epochs, \n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
        "  test_loss, test_accuracy = model.evaluate(xt, yt)\n",
        "  return model, history, test_accuracy\n",
        "\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRB51JO9xuxb"
      },
      "source": [
        "#Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm2YJbm9x5hP"
      },
      "source": [
        "# Get rid of unwanted characters and punctuation to reduce noise\n",
        "df_train[\"Sentence\"] = df_train[\"Sentence\"].map(clean_text)\n",
        "df_test[\"Sentence\"] = df_test[\"Sentence\"].map(clean_text)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "tEAr8Or6yTDw",
        "outputId": "24f70e10-71fd-46bc-f4f8-9d370a87c159"
      },
      "source": [
        "df_train.head()\n",
        "df_test.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>third quarter 2010 net sales increased 52 eur ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>foundries division reports sales increased 97 ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>financing project come mainly china</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sukhraj dulai 2900 block boni sue court culdes...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>finland leading metals group outokumpu said fo...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Sentence  Label\n",
              "0  third quarter 2010 net sales increased 52 eur ...      2\n",
              "1  foundries division reports sales increased 97 ...      2\n",
              "2                financing project come mainly china      1\n",
              "3  sukhraj dulai 2900 block boni sue court culdes...      1\n",
              "4  finland leading metals group outokumpu said fo...      2"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mn-01j10EK9"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 256\n",
        "MAX_NB_WORDS = 1000\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz4ZhVzz2IeX",
        "outputId": "eb54b641-56fc-499b-a46c-dde6f16ed3b7"
      },
      "source": [
        "# Pad X train and X test, with the model fitted to X train\n",
        "X_train, X_test = pad_sequences_train(df_train, df_test)\n",
        "X_train"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ..., 347,  91, 161],\n",
              "       [  0,   0,   0, ..., 285,  93, 484],\n",
              "       [  0,   0,   0, ...,  39, 185, 654],\n",
              "       ...,\n",
              "       [  0,   0,   0, ...,  32,  11,  16],\n",
              "       [  0,   0,   0, ...,  11,  94,  15],\n",
              "       [  0,   0,   0, ..., 909,  33, 218]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iE3DhuM2Q0c",
        "outputId": "c3d4a7e6-4f96-416f-bd39-b6be5902ad29"
      },
      "source": [
        "X_test"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   4,   1,   2],\n",
              "       [  0,   0,   0, ...,  32,  11,  31],\n",
              "       [  0,   0,   0, ..., 786, 534, 270],\n",
              "       ...,\n",
              "       [  0,   0,   0, ...,  25, 954, 363],\n",
              "       [  0,   0,   0, ..., 536, 931,  27],\n",
              "       [  0,   0,   0, ...,   2,   1,   2]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yxmfkKe2lmX"
      },
      "source": [
        "# Get dummies to turn labels into a format where it can be processed by a probabilistic model\n",
        "Y_train = pd.get_dummies(df_train[\"Label\"]).values\n",
        "Y_test = pd.get_dummies(df_test[\"Label\"]).values\n",
        "# Y_train"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z0jAg4724hm"
      },
      "source": [
        "# Y_test"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZRsUmLv3CAn",
        "outputId": "0df4261d-3816-447d-9aa4-71d5c1424c33"
      },
      "source": [
        "# Initialize model\n",
        "label_count = 3\n",
        "label_map = {0:\"negative\", 1:\"neutral\", 2:\"positive\"}\n",
        "\n",
        "model = run_model(X_train, Y_train, X_test, Y_test, label_count, 5, 256, 1000, 300)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 256, 300)          300300    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 256, 300)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 303       \n",
            "=================================================================\n",
            "Total params: 461,003\n",
            "Trainable params: 461,003\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "51/51 [==============================] - 90s 2s/step - loss: 0.8588 - accuracy: 0.6900 - val_loss: 0.6752 - val_accuracy: 0.7451\n",
            "Epoch 2/5\n",
            "51/51 [==============================] - 85s 2s/step - loss: 0.4770 - accuracy: 0.8158 - val_loss: 0.5440 - val_accuracy: 0.7892\n",
            "Epoch 3/5\n",
            "51/51 [==============================] - 84s 2s/step - loss: 0.3107 - accuracy: 0.8999 - val_loss: 0.5226 - val_accuracy: 0.8064\n",
            "Epoch 4/5\n",
            "51/51 [==============================] - 85s 2s/step - loss: 0.1908 - accuracy: 0.9454 - val_loss: 0.6181 - val_accuracy: 0.7990\n",
            "Epoch 5/5\n",
            "51/51 [==============================] - 84s 2s/step - loss: 0.1249 - accuracy: 0.9638 - val_loss: 0.6891 - val_accuracy: 0.7892\n",
            "8/8 [==============================] - 1s 131ms/step - loss: 0.5520 - accuracy: 0.8326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkks3agU33gx"
      },
      "source": [
        ""
      ],
      "execution_count": 34,
      "outputs": []
    }
  ]
}